{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMSE 586: Big Data Analytics and Visualization\n",
    "\n",
    "## Lecture 5 - Multiple linear regression and model validation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container{width:100%}</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "## $$Y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\cdots\\beta_px_p+\\epsilon$$\n",
    "\n",
    "## where $$\\epsilon \\sim \\text{N}(0, \\sigma^2)$$\n",
    "\n",
    "\n",
    "## Interpretation of $\\beta_i$: The expected change of $Y$ per unit change of $x_i$, holding all other predictors fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/advertising.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building a multiple linear regression model with `statsmodels`\n",
    "\n",
    "## $$\\text{sales}=\\beta_0+\\beta_1\\cdot \\text{TV} +\\beta_2\\cdot \\text{radio} +\\beta_3\\cdot \\text{newspaper}+\\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's drop the newspaper predictor in the model\n",
    "\n",
    "## $$\\text{sales}=\\beta_0+\\beta_1\\cdot \\text{TV} +\\beta_2\\cdot \\text{radio}+\\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Making predictions\n",
    "\n",
    "### What is the predicted sales with a TV budget of 180 and radio budget of 20?\n",
    "\n",
    "## $$\\text{E[sales]}=\\hat{\\beta}_0+\\hat{\\beta}_1\\cdot \\text{TV} +\\hat{\\beta}_2\\cdot \\text{radio}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interactions\n",
    "\n",
    "## $$\\text{sales}=\\beta_0+\\beta_1\\cdot \\text{TV} +\\beta_2\\cdot \\text{radio}+\\epsilon$$\n",
    "\n",
    "### For a 1-unit increase in <font color='red'>TV</font>, the average effect on sales is alwasy $\\beta_1$, regardless of the spendings in <font color='green'>radio</font>. \n",
    "\n",
    "### Similarly, for a 1-unit increase in <font color='green'>radio</font>, the average effect on sales is alwasy $\\beta_2$, regardless of the spendings in <font color='red'>TV</font>. \n",
    "\n",
    "### But maybe, with a higher spending on  <font color='green'>radio</font>, the <font color='red'>TV</font> spending can be more effective (i.e., with a steeper slope). \n",
    "\n",
    "### In other words, the effectiveness of the <font color='red'>TV</font> spending varies depending on the level of the <font color='green'>radio</font> spending. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marketing: synergy (effect)\n",
    "### Statistics: interaction (effect)\n",
    "\n",
    "## \\begin{align}\n",
    "\\text{sales}&=\\beta_0+\\beta_1\\cdot \\text{TV} +\\beta_2\\cdot \\text{radio}+\\beta_3\\cdot \\text{TV} \\cdot\\text{radio}+\\epsilon \\\\\n",
    "\\\\\n",
    "&=\\beta_0+(\\beta_1 + \\beta_3\\cdot\\text{radio}) \\cdot \\text{TV} +\\beta_2\\cdot \\text{radio}+\\epsilon \\\\\n",
    "\\\\\n",
    "&=\\beta_0+\\beta_1\\cdot \\text{TV} + (\\beta_2 + \\beta_1\\cdot\\text{TV}) \\cdot \\text{radio} +\\epsilon \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the **hierarchical principle** states that if we include an interaction in a model, we should also include the main effects, even if the *p*-values associated with their coefficients are not significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Categorical predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.read_csv('data/customer_credit.csv')\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Categorical predictors with <font color='red'><b>two</b></font> levels\n",
    "\n",
    "### We can create a dummy binary variable. For example, for the Student\n",
    "\n",
    "## \\begin{equation}\n",
    "  x =\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if student}\\\\\n",
    "      0, & \\text{if not student}\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "\n",
    "### Let's build a multiple linear regression model\n",
    "\n",
    "## $$\\text{Balance}=\\beta_0+\\beta_1\\cdot \\text{Income} +\\beta_2\\cdot \\text{Student}+\\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the interpretations of the slope parameter estimates $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add the interaction term\n",
    "\n",
    "## $$\\text{Balance}=\\beta_0+\\beta_1\\cdot\\text{Income}+\\beta_2\\cdot\\text{Student}+\\beta_3\\cdot\\text{Income}\\cdot\\text{Student}+\\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 2 Categorical predictors with more than two levels\n",
    "\n",
    "### We should NOT use a single dummy variable to represent all values.\n",
    "\n",
    "## \\begin{equation}\n",
    "  \\text{Ethnicity} =\n",
    "    \\begin{cases}\n",
    "      0, & \\text{if African American}\\\\\n",
    "      1, & \\text{if Asian}\\\\\n",
    "      2, & \\text{if Caucasian}\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### The reason is\n",
    "\n",
    "## \\begin{align}\n",
    "\\text{Balance}&=\\beta_0 + \\beta_1\\cdot\\text{Income} + \\beta_2\\cdot\\text{Ethnicity} + \\epsilon \\\\\n",
    "\\\\\n",
    "&=\n",
    "    \\begin{cases}\n",
    "      \\beta_0+ \\beta_1\\cdot\\text{Income}+ \\epsilon, & \\text{if African American}\\\\\n",
    "      (\\beta_0 + \\beta_2) + \\beta_1\\cdot\\text{Income}+ \\epsilon, & \\text{if Asian}\\\\\n",
    "      (\\beta_0 + 2\\beta_2) + \\beta_1\\cdot\\text{Income}+ \\epsilon, & \\text{if Caucasian}\n",
    "    \\end{cases}       \n",
    "\\end{align}\n",
    "\n",
    "### This essentially assigns **equal distance** (of $\\beta_2$) of the Balance between the ethnicity groups, which doesn't make sense. \n",
    "\n",
    "### The correct way to do it is to create <font color='red'><b>two</b></font> dummy binary variables\n",
    "\n",
    "## \\begin{equation}\n",
    "  \\text{Asian} =\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if Asian}\\\\\n",
    "      0, & \\text{If not Asian}\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "\n",
    "## \\begin{equation}\n",
    "  \\text{Caucasion} =\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if Caucasian}\\\\\n",
    "      0, & \\text{If not Caucasian}\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "\n",
    "### In this case, we have\n",
    "\n",
    "## \\begin{align}\n",
    "\\text{E[Balance]}&=\\beta_0 + \\beta_1\\cdot\\text{Income} + \\beta_2\\cdot\\text{Ethnicity} \\\\\n",
    "\\\\\n",
    "&=\n",
    "    \\begin{cases}\n",
    "      \\beta_0+ \\beta_1\\cdot\\text{Income}, & \\text{if African American}\\\\\n",
    "      (\\beta_0 + \\beta_2) + \\beta_1\\cdot\\text{Income}, & \\text{if Asian}\\\\\n",
    "      (\\beta_0 + \\beta_3) + \\beta_1\\cdot\\text{Income}, & \\text{if Caucasian}\n",
    "    \\end{cases}       \n",
    "\\end{align}\n",
    "\n",
    "### In the above example, we consider the African American as the baseline. \n",
    "### Its distance (in Balance) to Asian is $\\beta_2$, and to Caucasian is $\\beta_3$.\n",
    "\n",
    "### In general, there will always be <font color='red'><b>one fewer</b></font> dummy binary variable than the number of levels. \n",
    "\n",
    "### This is called [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) in machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build a multiple linear regression model\n",
    "\n",
    "### $$\\text{Balance}=\\beta_0+\\beta_1\\cdot \\text{Income} +\\beta_2\\cdot \\text{Asian}+\\beta_3\\cdot \\text{Caucasian}+\\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A practical tip: Dealing with categorical predictors that were coded in numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a new column to represent the case where a categorical variable was coded in numbers\n",
    "credit['Ethnicity_coded'] = credit['Ethnicity']\n",
    "\n",
    "replacement_dict = {'Ethnicity_coded': {'Asian': 111, \n",
    "                                        'Caucasian': 222,\n",
    "                                        'African American': 333}}\n",
    "\n",
    "credit.replace(replacement_dict, inplace = True)\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Non-linear relationship - polynomial regression\n",
    "\n",
    "## $$Y=\\beta_0+\\beta_1x+\\beta_2x^2+\\cdots+\\beta_px^p+\\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv('./data/auto.csv')\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Linear regression in scikit-learn `sklearn`\n",
    "\n",
    "### <a href=\"https://scikit-learn.org/stable/\">scikit-learn</a>  is a powerful and popular machine learning library. \n",
    "\n",
    "### Main differences of regresssion in `statsmodels` vs `sklearn`:\n",
    "\n",
    "### `statsmodels`: more traditional flavor, emphasize on analyzing how a given model fits the (training) data, provide more statistics (e.g., hypothesis testing). \n",
    "\n",
    "\n",
    "### `sklearn`: more machine learning flavor, emphasize on model selection (i.e., finding the \"best\" model for out-of-sample prediction) and cross-validataion on test data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on the scikit-learn `LinearRegression` function see [link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To model interactions, `patsy` library can help us to generate a feature matrix that includes them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scikit-learn feature matrix only takes numerical values. \n",
    "\n",
    "### To include categorical variables, we canuse the scikit-learn function `OneHotEncoder` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">(link)</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model evaluation metrics\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "<h3>$$\\text{MAE}=\\frac{1}{n}\\sum_{i=1}^{n} |y_i-\\hat{y}_i|$$</h3>\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "<h3>$$\\text{MSE}=\\frac{\\text{RSS}}{n}=\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2$$</h3>\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "<h3>$$\\text{RMSE}=\\sqrt{\\text{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}$$</h3>\n",
    "\n",
    "### $R^2$ (coefficient of determination)\n",
    "<h3>$$R^2=\\frac{\\text{TSS}-\\text{RSS}}{\\text{TSS}}=1-\\frac{\\text{RSS}}{\\text{TSS}}=1-\\frac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}$$</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some imaginary data\n",
    "y_true = [8, 5, 7, 10]    # true values of y from data\n",
    "y_pred = [6, 5, 10, 13]   # predicted values of y from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MAE by hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MSE by hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Overfitting\n",
    "\n",
    "### The model follows too closely to the data. \n",
    "\n",
    "### It is working too hard to find patterns in the data, and may be picking up some patterns that are just caused by noise, rather than the true characteristics of the underlying relationship. \n",
    "\n",
    "### Overfitting is undesirable because the fitted model will yield worse results on new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A demonstration of overfitting use some artificial data.\n",
    "### Assume the underlying true relationship is \n",
    "## <h2>$$y=x+x^2$$</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some artificial data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## $$R^2=1-\\frac{\\text{RSS}}{\\text{TSS}}$$\n",
    "\n",
    "\n",
    "### Adjusted $R^2$\n",
    "## $$R^2_{adj}=1-\\frac{\\text{RSS}/(n-p-1)}{\\text{TSS}/(n-1)}$$\n",
    "\n",
    "### where $p$ is the number of predictors. \n",
    "\n",
    "### If we replace $\\frac{\\text{RSS}}{\\text{TSS}}$ in the first equation above with $1-R^2$, we have\n",
    "\n",
    "## $$R^2_{adj}=1-(1-R^2)\\frac{(n-1)}{(n-p-1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the simulated data above, let's calculate the metrics for the fitted polynomial regression models with varying orders. \n",
    "\n",
    "## $$Y=\\beta_0+\\beta_1x+\\beta_2x^2+\\cdots+\\beta_px^p+\\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "MSE = []\n",
    "R2 = []\n",
    "R2_adj = []\n",
    "\n",
    "for p in np.arange(1, 10):\n",
    "    \n",
    "    poly = PolynomialFeatures(p)\n",
    "    X = poly.fit_transform(df_sim[['x']])\n",
    "\n",
    "    lr.fit(X, y)\n",
    "    \n",
    "    y_true = y\n",
    "\n",
    "    y_pred = lr.predict(X)\n",
    "\n",
    "    MSE.append(metrics.mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    rsquared = metrics.r2_score(y_true, y_pred)\n",
    "    R2.append(rsquared)\n",
    "    \n",
    "    n = len(df_sim)\n",
    "    rsquared_adj = 1 - (1-rsquared) * (n-1) / (n-p-1)\n",
    "    R2_adj.append(rsquared_adj)\n",
    "\n",
    "MSE\n",
    "# R2\n",
    "# R2_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train/test split\n",
    "\n",
    "### An alternative approach to find the best model is to split the data into two parts: \n",
    "- ### a training set\n",
    "- ### a test set. \n",
    "\n",
    "\n",
    "### Then we build the model using only the training set, and evaluate the model performance using only the test set. \n",
    "\n",
    "### Advantages over using the adjusted $R^2$\n",
    "\n",
    "- ### More straightforward, provides a direct estimate of the test error. \n",
    "\n",
    "- ### Make fewer assumptions about the true underlying model.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- ### More computationally expensive.\n",
    "\n",
    "- ### Train/test split may have high variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train/test sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build the model using only the training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In scikit-learn a random split can be done with the `train_test_split` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">(link)</a> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see from above, the train/test split method may has a high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cross-validation\n",
    "\n",
    "### Procedure:\n",
    "\n",
    "- ### The training set is split into $k$ groups, or *folds*, of approximately equal size.\n",
    "\n",
    "\n",
    "- ### A model is trained using $(k-1)$ of the folds as training data.\n",
    "\n",
    "\n",
    "- ### The resulting model is validated on the remaining fold. This fold is often called \"held out validation set\". \n",
    "\n",
    "\n",
    "- ### The performance measure reported by the *k*-fold cross-validation is the average of the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This image demonstrades a 10-fold cross validation procedure.\n",
    "\n",
    "![k-fold cross validataion](https://miro.medium.com/max/3115/1*me-aJdjnt3ivwAurYkB7PA.png)\n",
    "\n",
    "[Image source](https://medium.com/@sebastiannorena/some-model-tuning-methods-bfef3e6544f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on the scikit-learn `KFold` function see [link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cross-validated metrics\n",
    "\n",
    "### To use cross-validation in a more automatically way, we can use the scikit-learn `cross_val_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on the `cross_val_score` function see [link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "\n",
    "For more on the `scoring` argument of the function see [link](https://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the artificial data again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One final note on cross-validataion:\n",
    "### Once we use cross validation to choose the \"best\" model configuration (i.e., what features to include in the regression model), we then finalize the model (i.e., estimating the coefficients) by apply the selected model configuration on <font color = 'red'>all the data</font>. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
